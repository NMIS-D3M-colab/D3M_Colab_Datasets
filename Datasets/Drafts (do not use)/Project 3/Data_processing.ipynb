{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8ec4025",
   "metadata": {},
   "source": [
    "# Import and installation of Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80c06fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39468ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e6e4d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', 500) #This has helped me to see the data a bit more in order to see if my coding was effective. For pdf creation purpose, I have disabled this. \n",
    "#pd.set_option('display.max_columns', 500)\n",
    "#pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8150b5f3",
   "metadata": {},
   "source": [
    "# Read CSV file with all sensors Tags of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "152918cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors=pd.read_csv('sensors.csv', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faa2208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sensors_colnames.columns = [\"Sensors\", \"Condition\"]\n",
    "#sensors_colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18370cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_true = sensors_colnames[sensors_colnames['Condition']==True]['Sensors'].tolist()\n",
    "#list_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94b0022",
   "metadata": {},
   "source": [
    "# Main part of the code to read, process, append all the indivual csv file into a set of dataframe (one DF for each sensor at this stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8170c0a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10892\\4133003326.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#create empty DF within the dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mon_bad_lines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'skip'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m  \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_[0-9]*.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#issue with PID_ZONE_2_OUT fixed with _[0-9], skip bad lines needed as a very few rows are corrupted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# drop the first column which contains sensors name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Value'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#rename the column value with the name of sensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIndexes\u001b[0m \u001b[0mhave\u001b[0m \u001b[0moverlapping\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m     \"\"\"\n\u001b[1;32m--> 347\u001b[1;33m     op = _Concatenator(\n\u001b[0m\u001b[0;32m    348\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No objects to concatenate\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "start = time.time() #used to calculate processing time\n",
    "\n",
    "file_path = Path(\"Raw_furnace_data/\") #file path\n",
    "\n",
    "d = {} #creation of empty dictionary to store all DFs\n",
    "for i in sensors:\n",
    "    name = i\n",
    "    d[name] = pd.DataFrame() #create empty DF within the dictionary\n",
    "    d[name]= pd.concat([pd.read_csv(j,sep=';',on_bad_lines='skip') for  j in file_path.glob(str(name)+'_[0-9]*.csv')], ignore_index= True) #issue with PID_ZONE_2_OUT fixed with _[0-9], skip bad lines needed as a very few rows are corrupted\n",
    "    d[name].drop('Name', axis=1, inplace=True) # drop the first column which contains sensors name\n",
    "    d[name].rename(columns={'Value': str(name)}, inplace=True) #rename the column value with the name of sensor\n",
    "    d[name]['Date'] =  pd.to_datetime(d[name]['Date'], format='%d/%m/%Y %H:%M:%S', errors='coerce') #convert the date column to a python datetime format\n",
    "    d[name].set_index('Date', inplace=True) #set the newly formated date column as the index\n",
    "    d[name] = d[name].sort_index() # sort by index as it was noticed that some csv files contains some data from last year\n",
    "    d[name]=d[name].loc[d[name].index.notnull()] #\n",
    "    print (name)\n",
    "    print (d[name].info())\n",
    "    print (d[name].head())\n",
    "    \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea21a285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define start date and end date (we notice issue in the orginal csvs with csv containing data from previous years)\n",
    "#keep data between these two dates\n",
    "# this could have been included in previous operation but I have decided to separate it for clarity\n",
    "\n",
    "start_date='2022-09-06 09:00:00'\n",
    "end_date='2022-10-14 09:00:00'\n",
    "start = time.time()\n",
    "for i in list_true:\n",
    "    name = i\n",
    "    d[name] = d[name].loc[start_date:end_date]\n",
    "    print (name)\n",
    "    print (d[name].info())\n",
    "    print (d[name])\n",
    "    \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5ea545",
   "metadata": {},
   "source": [
    "# creating a list of DFs from the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7335090",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#make a combined list with all sensor names\n",
    "#the issue is that individuals have slightly different lengths\n",
    "\n",
    "DF_list= list()\n",
    "for i in list_true:\n",
    "    name = i\n",
    "    DF_list.append(d[name])\n",
    "\n",
    "DF_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb83512",
   "metadata": {},
   "source": [
    "# reducing dataset size using a formatted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45ea5b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#define sampling frequency\n",
    "#create empty df from start date and end date with correct sampling frequency\n",
    "\n",
    "sampling_frequency='10S'\n",
    "df_Date_filter = pd.DataFrame({'Date' : pd.date_range(start_date, end_date, freq=sampling_frequency)})\n",
    "df_Date_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64452a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a copy of the dictionary for the reduced data (between two dates)\n",
    "#merge with the reference df with defined sampling frequency\n",
    "#interpolate data for missing timestamp\n",
    "#there is a small issue as it creates duplicated index\n",
    "\n",
    "start = time.time()\n",
    "import functools as ft\n",
    "d_red = d.copy()\n",
    "for i in list_true:\n",
    "    name = i\n",
    "    d_red[name] = ft.reduce(lambda  left,right: pd.merge(left,right,on=['Date'],how='outer'), [df_Date_filter, d_red[name]])\n",
    "    d_red[name].set_index('Date', inplace=True)\n",
    "    d_red[name] = d_red[name].sort_index()\n",
    "    d_red[name] = d_red[name].interpolate()\n",
    "    print (name)\n",
    "    print (d_red[name].info())\n",
    "    print (d_red[name])\n",
    "    \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5750c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this step remove the duplicated indexes\n",
    "#the final DFs have all the same length\n",
    "\n",
    "start = time.time()\n",
    "for i in list_true:\n",
    "    name = i\n",
    "    d_red[name] = ft.reduce(lambda  left,right: pd.merge(left,right,on=['Date'],how='inner'), [df_Date_filter, d_red[name]])\n",
    "    d_red[name].set_index('Date', inplace=True)\n",
    "    d_red[name] = d_red[name].sort_index()\n",
    "    d_red[name] = d_red[name][~d_red[name].index.duplicated(keep='first')]\n",
    "    print (name)\n",
    "    print (d_red[name].info())\n",
    "    print (d_red[name])\n",
    "    \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd6782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_red['ROOF_0104_300_09_TC'] #exemple of DF for sensor 'ROOF_0104_300_09_TC'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49df096e",
   "metadata": {},
   "source": [
    "# creating a list of the reduced DFs from the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f7a583",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DF_list_red = list()\n",
    "for i in list_true:\n",
    "    name = i\n",
    "    DF_list_red.append(d_red[name])\n",
    "\n",
    "DF_list_red"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9d75e5",
   "metadata": {},
   "source": [
    "# This is the  step to merge all the individuals DFs into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b315cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "#import functools as ft\n",
    "df_merged_red=DF_list_red[0].copy()\n",
    "\n",
    "for i in range(1, len(DF_list_red), 1):\n",
    "    df_merged_red = df_merged_red.merge(DF_list_red[i],on=['Date'],how='left')\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4150c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aab61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_red.info() #we managed to reduced the 5.71 GB datset to a single DF of 90 MB (99.5% reduction in memory usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b234757",
   "metadata": {},
   "source": [
    "# this step is to export the resulting merged DF to a pickled file for the following plotting and anlaysis operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46184b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_red.to_pickle(\"C:/AURIK/02b_Training/67 - Data_Science_for_Manufacturing_September2022/DS4M Assignment 2 Dec 22/Pickle/df_merged_red.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94d76f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
